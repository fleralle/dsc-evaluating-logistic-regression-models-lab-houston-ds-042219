{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Logistic Regression Models - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    " In regression, you are predicting values so it made sense to discuss error as a distance of how far off our estimates were. In classifying a binary variable however, a model is either correct or incorrect. As a result, we tend to deconstruct this as how many false positives versus false negatives we come across.  \n",
    "In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this review lab, we'll review precision, recall, accuracy, and F1-score in order to evaluate our logistic regression models.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "You will be able to:  \n",
    "* Understand and assess precision, recall, and accuracy of classifiers\n",
    "* Evaluate classification models using various metrics\n",
    "\n",
    "## Terminology Review  \n",
    "\n",
    "Let's take a moment and review some classification evaluation metrics:  \n",
    "\n",
    "\n",
    "$Precision = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}}$    \n",
    "  \n",
    "\n",
    "$Recall = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}}$  \n",
    "  \n",
    "$Accuracy = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}}$\n",
    "\n",
    "$\\text{F1-Score} = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At times, it may be superior to tune a classification algorithm to optimize against precision or recall rather than overall accuracy. For example, imagine the scenario of predicting whether or not a patient is at risk for cancer and should be brought in for additional testing. In cases such as this, we often may want to cast a slightly wider net, and it is much preferable to optimize for recall, the number of cancer positive cases, then it is to optimize precision, the percentage of our predicted cancer-risk patients who are indeed positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df.iloc[:,-1]\n",
    "X = df.iloc[:,:-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a standard logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e15, fit_intercept=False)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_train_predicted = logreg.predict(X_train)\n",
    "y_test_predicted = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a function to calculate the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8527131782945736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(y_hat, y):\n",
    "    combos = list(zip(y_hat, y))\n",
    "    true_positive = sum([ (y_pred==y_true and y_true) for y_pred, y_true in combos])\n",
    "    total_predicted_true_positive = sum([ y_pred for y_pred, y_true in combos])\n",
    "    return true_positive / total_predicted_true_positive\n",
    "\n",
    "precision(y_train_predicted, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write a function to calculate the recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9243697478991597"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recall(y_hat, y):\n",
    "    combos = list(zip(y_hat, y))\n",
    "    true_positive = sum([ (y_pred==y_true and y_true) for y_pred, y_true in combos])\n",
    "    total_actual_positive = sum([ y_true for y_pred, y_true in combos])\n",
    "    return true_positive / total_actual_positive\n",
    "\n",
    "recall(y_train_predicted, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8679245283018868"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    combos = list(zip(y_hat, y))\n",
    "    true_positive = sum([ (y_pred==y_true and y_true) for y_pred, y_true in combos])\n",
    "    true_negative = sum([ (y_pred==y_true and y_true==False) for y_pred, y_true in combos])\n",
    "    total_observations = len(y)\n",
    "    return (true_positive + true_negative) / total_observations\n",
    "\n",
    "accuracy(y_train_predicted, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write a function to calculate the F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8870967741935484"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_score(y_hat,y):\n",
    "    prec = precision(y_hat, y)\n",
    "    rec = recall(y_hat, y)\n",
    "    \n",
    "    return 2*prec*rec / (prec + rec)\n",
    "\n",
    "f1_score(y_train_predicted, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate the precision, recall, accuracy, and F1-score of your classifier.\n",
    "\n",
    "Do this for both the training and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Train set --------\n",
      "precision:  0.8527131782945736\n",
      "recall:  0.9243697478991597\n",
      "accuracy:  0.8679245283018868\n",
      "f1_score:  0.8870967741935484\n",
      "-------- Test set --------\n",
      "precision:  0.7959183673469388\n",
      "recall:  0.8478260869565217\n",
      "accuracy:  0.8131868131868132\n",
      "f1_score:  0.8210526315789473\n"
     ]
    }
   ],
   "source": [
    "train_precision = precision(y_train_predicted, y_train)\n",
    "train_recall = recall(y_train_predicted, y_train)\n",
    "train_accuracy = accuracy(y_train_predicted, y_train)\n",
    "train_f1_score = f1_score(y_train_predicted, y_train)\n",
    "\n",
    "test_precision = precision(y_test_predicted, y_test)\n",
    "test_recall = recall(y_test_predicted, y_test)\n",
    "test_accuracy = accuracy(y_test_predicted, y_test)\n",
    "test_f1_score = f1_score(y_test_predicted, y_test)\n",
    "\n",
    "print('-------- Train set --------')\n",
    "print('precision: ', train_precision)\n",
    "print('recall: ', train_recall)\n",
    "print('accuracy: ', train_accuracy)\n",
    "print('f1_score: ', train_f1_score)\n",
    "print('-------- Test set --------')\n",
    "print('precision: ', test_precision)\n",
    "print('recall: ', test_recall)\n",
    "print('accuracy: ', test_accuracy)\n",
    "print('f1_score: ', test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Job! Now it's time to check your work with sklearn. \n",
    "\n",
    "## 8. Calculating Metrics with sklearn\n",
    "\n",
    "Each of the metrics we calculated above is also available inside the `sklearn.metrics` module.  \n",
    "\n",
    "In the cell below, import the following functions:\n",
    "\n",
    "* `precision_score`\n",
    "* `recall_score`\n",
    "* `accuracy_score`\n",
    "* `f1_score`\n",
    "\n",
    "Compare the results of your performance metrics functions with the sklearn functions above. Calculate these values for both your train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Train set --------\n",
      "precision:  0.8527131782945736\n",
      "recall:  0.9243697478991597\n",
      "accuracy:  0.8679245283018868\n",
      "f1_score:  0.8870967741935484\n",
      "-------- Test set --------\n",
      "precision:  0.7959183673469388\n",
      "recall:  0.8478260869565217\n",
      "accuracy:  0.8131868131868132\n",
      "f1_score:  0.8210526315789473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "print('-------- Train set --------')\n",
    "print('precision: ', precision_score(y_train, y_train_predicted))\n",
    "print('recall: ', recall_score(y_train, y_train_predicted))\n",
    "print('accuracy: ', accuracy_score(y_train, y_train_predicted))\n",
    "print('f1_score: ', f1_score(y_train, y_train_predicted))\n",
    "print('-------- Test set --------')\n",
    "print('precision: ', precision_score(y_test, y_test_predicted))\n",
    "print('recall: ', recall_score(y_test, y_test_predicted))\n",
    "print('accuracy: ', accuracy_score(y_test, y_test_predicted))\n",
    "print('f1_score: ', f1_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparing Precision, Recall, Accuracy, and F1-Score of Test vs Train Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and then plot the precision, recall, accuracy, and F1-score for the test and train splits using different train set sizes. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_Precision = []\n",
    "testing_Precision = []\n",
    "training_Recall = []\n",
    "testing_Recall = []\n",
    "training_Accuracy = []\n",
    "testing_Accuracy = []\n",
    "\n",
    "for i in range(10,95):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3) #replace the \"None\" here\n",
    "    logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "    model_log = None\n",
    "    y_hat_test = None\n",
    "    y_hat_train = None\n",
    "\n",
    "# 6 lines of code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 4 scatter plots looking at the test and train precision in the first one, test and train recall in the second one, testing and training accuracy in the third one, and testing and training f1-score in the fourth one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for test and train precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for test and train recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for test and train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for test and train F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Nice! In this lab, you gained some extra practice with evaluation metrics for classification algorithms. You also got some further python practice by manually coding these functions yourself, giving you a deeper understanding of how they work. Going forward, continue to think about scenarios in which you might prefer to optimize one of these metrics over another."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
